<html>
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>威伦特</title>
<link rel="shortcut icon" href="https://voluntexi.github.io//favicon.ico?v=1769005591865">
<link href="https://cdn.jsdelivr.net/npm/remixicon@2.3.0/fonts/remixicon.css" rel="stylesheet">
<link rel="stylesheet" href="https://voluntexi.github.io//styles/main.css">
<link rel="alternate" type="application/atom+xml" title="威伦特 - Atom Feed" href="https://voluntexi.github.io//atom.xml">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700">



    <meta name="description" content=" Welcome to my Blog and it's my intention it will breed knowledge. " />
  </head>
  <body>
    <div class="main">
      <div class="main-content">
        <div class="site-header">
  <a href="https://voluntexi.github.io/">
  <img class="avatar" src="https://voluntexi.github.io//images/avatar.png?v=1769005591865" alt="">
  </a>
  <h1 class="site-title">
    威伦特
  </h1>
  <p class="site-description">
    解码生命
  </p>
  <div class="menu-container">
    
      
        <a href="/" class="menu">
          首页
        </a>
      
    
      
        <a href="/archives" class="menu">
          文章
        </a>
      
    
      
        <a href="/tags" class="menu">
          标签
        </a>
      
    
      
        <a href="/about" class="menu">
          关于
        </a>
      
    
  </div>
  <div class="social-container">
    
      
    
      
    
      
    
      
    
      
    
  </div>
</div>

    
        <div class="post-container">
  
    <article class="post">
      <a href="https://voluntexi.github.io/prompt-learning/">
        <h2 class="post-title">Prompt Learning</h2>
      </a>
      <div class="post-info">
        <span>
          2023-12-12
        </span>
        <span>
          9 min read
        </span>
        
          <a href="https://voluntexi.github.io/Non44iZPB/" class="post-tag">
            # NLP
          </a>
        
      </div>
      
        <a href="https://voluntexi.github.io/prompt-learning/" class="post-feature-image" style="background-image: url('https://miro.medium.com/v2/resize:fit:1100/format:webp/0*gJlUvCo-c1r0vbtz.png')">
        </a>
      
      <div class="post-abstract">
        <p><strong>Prompt Learning 的本质就是将所有下游任务统一成预训练任务；</strong> 以特定的模板，将下游任务的数据转成自然语言形式，从而充分挖掘预训练语言模型本身的能力。</p>

      </div>
    </article>
  
    <article class="post">
      <a href="https://voluntexi.github.io/less-is-more/">
        <h2 class="post-title">Less is More for Long Document Summary Evaluation by LLMs</h2>
      </a>
      <div class="post-info">
        <span>
          2023-10-09
        </span>
        <span>
          4 min read
        </span>
        
          <a href="https://voluntexi.github.io/wen-ben-zhai-yao/" class="post-tag">
            # 文本摘要
          </a>
        
          <a href="https://voluntexi.github.io/Non44iZPB/" class="post-tag">
            # NLP
          </a>
        
      </div>
      
        <a href="https://voluntexi.github.io/less-is-more/" class="post-feature-image" style="background-image: url('https://voluntexi.github.io//post-images/less-is-more.png')">
        </a>
      
      <div class="post-abstract">
        <p>这篇文章给了我们一种如何在自己研究的领域去&quot;蹭&quot;大模型热度的思路</p>

      </div>
    </article>
  
    <article class="post">
      <a href="https://voluntexi.github.io/PGA/">
        <h2 class="post-title">Generating EDU Extracts for Plan-Guided Summary Re-Ranking</h2>
      </a>
      <div class="post-info">
        <span>
          2023-09-09
        </span>
        <span>
          7 min read
        </span>
        
          <a href="https://voluntexi.github.io/wen-ben-zhai-yao/" class="post-tag">
            # 文本摘要
          </a>
        
          <a href="https://voluntexi.github.io/Non44iZPB/" class="post-tag">
            # NLP
          </a>
        
      </div>
      
        <a href="https://voluntexi.github.io/PGA/" class="post-feature-image" style="background-image: url('https://voluntexi.github.io//post-images/PGA.png')">
        </a>
      
      <div class="post-abstract">
        <p>这篇文章是在我之前介绍的<strong>BRIO模型（<a href="https://voluntexi.github.io/brio/">BRIO | 威伦特 (voluntexi.github.io)</a>）<strong>的基础上改进的，模型的整体框架也是采用两步式摘要，即结合</strong>生成候选摘要</strong>和<strong>评估候选摘要</strong>两个阶段来获得最佳摘要。</p>

      </div>
    </article>
  
    <article class="post">
      <a href="https://voluntexi.github.io/copy-is-all-you-need/">
        <h2 class="post-title">Copy is All You Need</h2>
      </a>
      <div class="post-info">
        <span>
          2023-08-21
        </span>
        <span>
          13 min read
        </span>
        
          <a href="https://voluntexi.github.io/Non44iZPB/" class="post-tag">
            # NLP
          </a>
        
      </div>
      
        <a href="https://voluntexi.github.io/copy-is-all-you-need/" class="post-feature-image" style="background-image: url('https://voluntexi.github.io//post-images/copy-is-all-you-need.png')">
        </a>
      
      <div class="post-abstract">
        <p>最近在<a href="https://paperswithcode.com/">paper with code</a>刷论文的时候，看到了一个很唬人的文章“《Copy is All You Need》”，遂找来研读研读，发现内容还是很有意思，准备写一篇阅读笔记的，偶然发现了这篇文章作者的采访稿，将文章背后的故事都介绍的挺详细的。于是乎转载一下（不是偷懒）</p>

      </div>
    </article>
  
    <article class="post">
      <a href="https://voluntexi.github.io/longnet/">
        <h2 class="post-title"> LONGNET: Scaling Transformers to 1,000,000,000 Tokens</h2>
      </a>
      <div class="post-info">
        <span>
          2023-07-21
        </span>
        <span>
          5 min read
        </span>
        
          <a href="https://voluntexi.github.io/Non44iZPB/" class="post-tag">
            # NLP
          </a>
        
      </div>
      
        <a href="https://voluntexi.github.io/longnet/" class="post-feature-image" style="background-image: url('https://voluntexi.github.io//post-images/longnet.png')">
        </a>
      
      <div class="post-abstract">
        <p>前段时间刚介绍了能使模型处理上下文扩展到百万级别的方法，现在微软又提出了一种能扩展到十亿级别的方法（不过有标题党的嫌疑，因为在实验中作者只扩展到了百万级别）</p>

      </div>
    </article>
  
    <article class="post">
      <a href="https://voluntexi.github.io/scalingTo1m/">
        <h2 class="post-title">Scaling Transformer to 1M tokens and beyond with RMT</h2>
      </a>
      <div class="post-info">
        <span>
          2023-07-07
        </span>
        <span>
          6 min read
        </span>
        
          <a href="https://voluntexi.github.io/Non44iZPB/" class="post-tag">
            # NLP
          </a>
        
      </div>
      
        <a href="https://voluntexi.github.io/scalingTo1m/" class="post-feature-image" style="background-image: url('https://voluntexi.github.io//post-images/scalingTo1m.png')">
        </a>
      
      <div class="post-abstract">
        <p>当我还在用最大一次只能处理1024个上下文的BART模型做实验时，已经有能处理上百万上下文的方法了🤡</p>

      </div>
    </article>
  
    <article class="post">
      <a href="https://voluntexi.github.io/lora/">
        <h2 class="post-title">LoRA</h2>
      </a>
      <div class="post-info">
        <span>
          2023-06-04
        </span>
        <span>
          10 min read
        </span>
        
          <a href="https://voluntexi.github.io/Non44iZPB/" class="post-tag">
            # NLP
          </a>
        
      </div>
      
        <a href="https://voluntexi.github.io/lora/" class="post-feature-image" style="background-image: url('https://p8.itc.cn/q_70/images03/20220319/d6bfb1e2afe6495a9252d2d545009dc6.png')">
        </a>
      
      <div class="post-abstract">
        <p>在如今大模型时代，如果需要微调一个大模型无疑在时间和金钱方面的消耗是巨大的，而LoRA通过冻结了预训练的模型权重，并将可训练的秩分解矩阵注入到Transformer架构的每一层中，大大减少了下游任务的可训练参数的数量。尽管LoRA使得可训练参数更少，但是与微调效果相比结果相当甚至更好。</p>

      </div>
    </article>
  
    <article class="post">
      <a href="https://voluntexi.github.io/longformer/">
        <h2 class="post-title">Longformer</h2>
      </a>
      <div class="post-info">
        <span>
          2023-05-18
        </span>
        <span>
          6 min read
        </span>
        
          <a href="https://voluntexi.github.io/Non44iZPB/" class="post-tag">
            # NLP
          </a>
        
      </div>
      
        <a href="https://voluntexi.github.io/longformer/" class="post-feature-image" style="background-image: url('https://pic4.zhimg.com/v2-f903a672ed4c3e0b41ec4ee89a2c8060_1440w.jpg?source=172ae18b')">
        </a>
      
      <div class="post-abstract">
        <p>Longformer是一种用来拓展模型在长序列建模的能力算法，它提出了一种时空复杂度同文本序列长度呈线性关系的Self-Attention，用以保证能够使得模型高效处理长文本。</p>

      </div>
    </article>
  
    <article class="post">
      <a href="https://voluntexi.github.io/SimCSE/">
        <h2 class="post-title">SimCSE</h2>
      </a>
      <div class="post-info">
        <span>
          2023-04-27
        </span>
        <span>
          6 min read
        </span>
        
          <a href="https://voluntexi.github.io/Non44iZPB/" class="post-tag">
            # NLP
          </a>
        
      </div>
      
        <a href="https://voluntexi.github.io/SimCSE/" class="post-feature-image" style="background-image: url('https://voluntexi.github.io//post-images/SimCSE.png')">
        </a>
      
      <div class="post-abstract">
        <p>最近做实验需要用到Sentence  Embeddings（句向量），特地研究了一下句向量相关模型算法，其中 SimCSE 模型是目前比较火、效果也比较好的一个模型。</p>

      </div>
    </article>
  
    <article class="post">
      <a href="https://voluntexi.github.io/Survey2/">
        <h2 class="post-title"> An Empirical Survey on Long Document Summarization,Part 2：Model</h2>
      </a>
      <div class="post-info">
        <span>
          2023-04-15
        </span>
        <span>
          16 min read
        </span>
        
          <a href="https://voluntexi.github.io/wen-ben-zhai-yao/" class="post-tag">
            # 文本摘要
          </a>
        
          <a href="https://voluntexi.github.io/Non44iZPB/" class="post-tag">
            # NLP
          </a>
        
      </div>
      
        <a href="https://voluntexi.github.io/Survey2/" class="post-feature-image" style="background-image: url('https://voluntexi.github.io//post-images/Survey2.png')">
        </a>
      
      <div class="post-abstract">
        <p>本文是论文《An Empirical Survey on Long Document Summarization》的阅读笔记第二部分，介绍了抽取式、生成式和混合式三种长文本摘要方法及其对应有哪些代表模型。</p>

      </div>
    </article>
  
</div>

    
        <div class="pagination-container">
  
    <a href="https://voluntexi.github.io/" class="prev-page">上一页</a>
  
  
    <a href="https://voluntexi.github.io/page/3/" class="next-page">下一页</a>
  
</div>

    
        <div class="site-footer">
  属于 <a href="https://github.com/voluntexi" target="_blank">@威伦特</a><script async defer src="https://analytics.umami.is/script.js" data-website-id="95248820-3fb8-420e-8f5b-87e136cbc08d"></script>
  <a class="rss" href="https://voluntexi.github.io//atom.xml" target="_blank">
    <i class="ri-rss-line"></i> RSS
  </a>
</div>

      </div>
    </div>
  </body>
</html>
