<html>
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>威伦特</title>
<link rel="shortcut icon" href="https://voluntexi.github.io//favicon.ico?v=1760695118645">
<link href="https://cdn.jsdelivr.net/npm/remixicon@2.3.0/fonts/remixicon.css" rel="stylesheet">
<link rel="stylesheet" href="https://voluntexi.github.io//styles/main.css">
<link rel="alternate" type="application/atom+xml" title="威伦特 - Atom Feed" href="https://voluntexi.github.io//atom.xml">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700">



    <meta name="description" content=" Welcome to my Blog and it's my intention it will breed knowledge. " />
  </head>
  <body>
    <div class="main">
      <div class="main-content">
        <div class="site-header">
  <a href="https://voluntexi.github.io/">
  <img class="avatar" src="https://voluntexi.github.io//images/avatar.png?v=1760695118645" alt="">
  </a>
  <h1 class="site-title">
    威伦特
  </h1>
  <p class="site-description">
    解码生命
  </p>
  <div class="menu-container">
    
      
        <a href="/" class="menu">
          首页
        </a>
      
    
      
        <a href="/archives" class="menu">
          文章
        </a>
      
    
      
        <a href="/tags" class="menu">
          标签
        </a>
      
    
      
        <a href="/about" class="menu">
          关于
        </a>
      
    
  </div>
  <div class="social-container">
    
      
    
      
    
      
    
      
    
      
    
  </div>
</div>

    
        <div class="post-container">
  
    <article class="post">
      <a href="https://voluntexi.github.io/copy-is-all-you-need/">
        <h2 class="post-title">Copy is All You Need</h2>
      </a>
      <div class="post-info">
        <span>
          2023-08-21
        </span>
        <span>
          13 min read
        </span>
        
          <a href="https://voluntexi.github.io/Non44iZPB/" class="post-tag">
            # NLP
          </a>
        
      </div>
      
        <a href="https://voluntexi.github.io/copy-is-all-you-need/" class="post-feature-image" style="background-image: url('https://voluntexi.github.io//post-images/copy-is-all-you-need.png')">
        </a>
      
      <div class="post-abstract">
        <p>最近在<a href="https://paperswithcode.com/">paper with code</a>刷论文的时候，看到了一个很唬人的文章“《Copy is All You Need》”，遂找来研读研读，发现内容还是很有意思，准备写一篇阅读笔记的，偶然发现了这篇文章作者的采访稿，将文章背后的故事都介绍的挺详细的。于是乎转载一下（不是偷懒）</p>

      </div>
    </article>
  
    <article class="post">
      <a href="https://voluntexi.github.io/longnet/">
        <h2 class="post-title"> LONGNET: Scaling Transformers to 1,000,000,000 Tokens</h2>
      </a>
      <div class="post-info">
        <span>
          2023-07-21
        </span>
        <span>
          5 min read
        </span>
        
          <a href="https://voluntexi.github.io/Non44iZPB/" class="post-tag">
            # NLP
          </a>
        
      </div>
      
        <a href="https://voluntexi.github.io/longnet/" class="post-feature-image" style="background-image: url('https://voluntexi.github.io//post-images/longnet.png')">
        </a>
      
      <div class="post-abstract">
        <p>前段时间刚介绍了能使模型处理上下文扩展到百万级别的方法，现在微软又提出了一种能扩展到十亿级别的方法（不过有标题党的嫌疑，因为在实验中作者只扩展到了百万级别）</p>

      </div>
    </article>
  
    <article class="post">
      <a href="https://voluntexi.github.io/scalingTo1m/">
        <h2 class="post-title">Scaling Transformer to 1M tokens and beyond with RMT</h2>
      </a>
      <div class="post-info">
        <span>
          2023-07-07
        </span>
        <span>
          6 min read
        </span>
        
          <a href="https://voluntexi.github.io/Non44iZPB/" class="post-tag">
            # NLP
          </a>
        
      </div>
      
        <a href="https://voluntexi.github.io/scalingTo1m/" class="post-feature-image" style="background-image: url('https://voluntexi.github.io//post-images/scalingTo1m.png')">
        </a>
      
      <div class="post-abstract">
        <p>当我还在用最大一次只能处理1024个上下文的BART模型做实验时，已经有能处理上百万上下文的方法了🤡</p>

      </div>
    </article>
  
    <article class="post">
      <a href="https://voluntexi.github.io/lora/">
        <h2 class="post-title">LoRA</h2>
      </a>
      <div class="post-info">
        <span>
          2023-06-04
        </span>
        <span>
          10 min read
        </span>
        
          <a href="https://voluntexi.github.io/Non44iZPB/" class="post-tag">
            # NLP
          </a>
        
      </div>
      
        <a href="https://voluntexi.github.io/lora/" class="post-feature-image" style="background-image: url('https://p8.itc.cn/q_70/images03/20220319/d6bfb1e2afe6495a9252d2d545009dc6.png')">
        </a>
      
      <div class="post-abstract">
        <p>在如今大模型时代，如果需要微调一个大模型无疑在时间和金钱方面的消耗是巨大的，而LoRA通过冻结了预训练的模型权重，并将可训练的秩分解矩阵注入到Transformer架构的每一层中，大大减少了下游任务的可训练参数的数量。尽管LoRA使得可训练参数更少，但是与微调效果相比结果相当甚至更好。</p>

      </div>
    </article>
  
    <article class="post">
      <a href="https://voluntexi.github.io/longformer/">
        <h2 class="post-title">Longformer</h2>
      </a>
      <div class="post-info">
        <span>
          2023-05-18
        </span>
        <span>
          6 min read
        </span>
        
          <a href="https://voluntexi.github.io/Non44iZPB/" class="post-tag">
            # NLP
          </a>
        
      </div>
      
        <a href="https://voluntexi.github.io/longformer/" class="post-feature-image" style="background-image: url('https://pic4.zhimg.com/v2-f903a672ed4c3e0b41ec4ee89a2c8060_1440w.jpg?source=172ae18b')">
        </a>
      
      <div class="post-abstract">
        <p>Longformer是一种用来拓展模型在长序列建模的能力算法，它提出了一种时空复杂度同文本序列长度呈线性关系的Self-Attention，用以保证能够使得模型高效处理长文本。</p>

      </div>
    </article>
  
    <article class="post">
      <a href="https://voluntexi.github.io/SimCSE/">
        <h2 class="post-title">SimCSE</h2>
      </a>
      <div class="post-info">
        <span>
          2023-04-27
        </span>
        <span>
          6 min read
        </span>
        
          <a href="https://voluntexi.github.io/Non44iZPB/" class="post-tag">
            # NLP
          </a>
        
      </div>
      
        <a href="https://voluntexi.github.io/SimCSE/" class="post-feature-image" style="background-image: url('https://voluntexi.github.io//post-images/SimCSE.png')">
        </a>
      
      <div class="post-abstract">
        <p>最近做实验需要用到Sentence  Embeddings（句向量），特地研究了一下句向量相关模型算法，其中 SimCSE 模型是目前比较火、效果也比较好的一个模型。</p>

      </div>
    </article>
  
    <article class="post">
      <a href="https://voluntexi.github.io/Survey2/">
        <h2 class="post-title"> An Empirical Survey on Long Document Summarization,Part 2：Model</h2>
      </a>
      <div class="post-info">
        <span>
          2023-04-15
        </span>
        <span>
          16 min read
        </span>
        
          <a href="https://voluntexi.github.io/wen-ben-zhai-yao/" class="post-tag">
            # 文本摘要
          </a>
        
          <a href="https://voluntexi.github.io/Non44iZPB/" class="post-tag">
            # NLP
          </a>
        
      </div>
      
        <a href="https://voluntexi.github.io/Survey2/" class="post-feature-image" style="background-image: url('https://voluntexi.github.io//post-images/Survey2.png')">
        </a>
      
      <div class="post-abstract">
        <p>本文是论文《An Empirical Survey on Long Document Summarization》的阅读笔记第二部分，介绍了抽取式、生成式和混合式三种长文本摘要方法及其对应有哪些代表模型。</p>

      </div>
    </article>
  
    <article class="post">
      <a href="https://voluntexi.github.io/Survey1/">
        <h2 class="post-title">An Empirical Survey on Long Document Summarization,Part 1：Introduction and Datasets</h2>
      </a>
      <div class="post-info">
        <span>
          2023-04-12
        </span>
        <span>
          8 min read
        </span>
        
          <a href="https://voluntexi.github.io/wen-ben-zhai-yao/" class="post-tag">
            # 文本摘要
          </a>
        
          <a href="https://voluntexi.github.io/Non44iZPB/" class="post-tag">
            # NLP
          </a>
        
      </div>
      
        <a href="https://voluntexi.github.io/Survey1/" class="post-feature-image" style="background-image: url('https://voluntexi.github.io//post-images/Survey1.png')">
        </a>
      
      <div class="post-abstract">
        <p>论文《An Empirical Survey on Long Document Summarization》对长文本摘要领域通过模型、数据集和评价指标三个方面进行了全面的概述，文本是该论文阅读笔记第一部分，描述了长文本的概念，介绍了目前的数据集。</p>

      </div>
    </article>
  
    <article class="post">
      <a href="https://voluntexi.github.io/dui-yu-xun-lian-mo-xing-jin-xing-wei-diao/">
        <h2 class="post-title">对预训练模型进行微调</h2>
      </a>
      <div class="post-info">
        <span>
          2023-03-01
        </span>
        <span>
          7 min read
        </span>
        
          <a href="https://voluntexi.github.io/Non44iZPB/" class="post-tag">
            # NLP
          </a>
        
      </div>
      
        <a href="https://voluntexi.github.io/dui-yu-xun-lian-mo-xing-jin-xing-wei-diao/" class="post-feature-image" style="background-image: url('https://img1.baidu.com/it/u=1732097677,3720483819&amp;fm=253&amp;fmt=auto&amp;app=138&amp;f=PNG?w=500&amp;h=208')">
        </a>
      
      <div class="post-abstract">
        <p>​	近年来随着自然语言处理技术的不断发展，预训练模型已经成为了近年来最热门的研究方向之一。预训练模型有更好的性能表现。然而，对于刚接触的人来说，阵对预训练模型的训练可能会显得复杂和难以理解。</p>

      </div>
    </article>
  
    <article class="post">
      <a href="https://voluntexi.github.io/moca/">
        <h2 class="post-title">MoCa</h2>
      </a>
      <div class="post-info">
        <span>
          2023-02-22
        </span>
        <span>
          7 min read
        </span>
        
          <a href="https://voluntexi.github.io/Non44iZPB/" class="post-tag">
            # NLP
          </a>
        
      </div>
      
        <a href="https://voluntexi.github.io/moca/" class="post-feature-image" style="background-image: url('https://voluntexi.github.io//post-images/moca.jpg')">
        </a>
      
      <div class="post-abstract">
        <p>BRIO在生成式文本摘要领域SOTA位置还没坐稳几个月，便出现了新的SOTA—MoCa</p>

      </div>
    </article>
  
</div>

    
        <div class="pagination-container">
  
    <a href="https://voluntexi.github.io/" class="prev-page">上一页</a>
  
  
    <a href="https://voluntexi.github.io/page/3/" class="next-page">下一页</a>
  
</div>

    
        <div class="site-footer">
  属于 <a href="https://github.com/voluntexi" target="_blank">@威伦特</a><script async defer src="https://analytics.umami.is/script.js" data-website-id="95248820-3fb8-420e-8f5b-87e136cbc08d"></script>
  <a class="rss" href="https://voluntexi.github.io//atom.xml" target="_blank">
    <i class="ri-rss-line"></i> RSS
  </a>
</div>

      </div>
    </div>
  </body>
</html>
