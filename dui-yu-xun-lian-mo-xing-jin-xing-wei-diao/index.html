<html>
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>对预训练模型进行微调 | 威伦特</title>
<link rel="shortcut icon" href="https://voluntexi.github.io//favicon.ico?v=1760695118645">
<link href="https://cdn.jsdelivr.net/npm/remixicon@2.3.0/fonts/remixicon.css" rel="stylesheet">
<link rel="stylesheet" href="https://voluntexi.github.io//styles/main.css">
<link rel="alternate" type="application/atom+xml" title="对预训练模型进行微调 | 威伦特 - Atom Feed" href="https://voluntexi.github.io//atom.xml">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700">



    <meta name="description" content="​	近年来随着自然语言处理技术的不断发展，预训练模型已经成为了近年来最热门的研究方向之一。预训练模型有更好的性能表现。然而，对于刚接触的人来说，阵对预训练模型的训练可能会显得复杂和难以理解。

​	因此，本文将以对BART微调用于文本摘要任..." />
    <meta name="keywords" content="NLP" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
    <script src="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.5.1/build/highlight.min.js"></script>
  </head>
  <body>
    <div class="main">
      <div class="main-content">
        <div class="site-header">
  <a href="https://voluntexi.github.io/">
  <img class="avatar" src="https://voluntexi.github.io//images/avatar.png?v=1760695118645" alt="">
  </a>
  <h1 class="site-title">
    威伦特
  </h1>
  <p class="site-description">
    解码生命
  </p>
  <div class="menu-container">
    
      
        <a href="/" class="menu">
          首页
        </a>
      
    
      
        <a href="/archives" class="menu">
          文章
        </a>
      
    
      
        <a href="/tags" class="menu">
          标签
        </a>
      
    
      
        <a href="/about" class="menu">
          关于
        </a>
      
    
  </div>
  <div class="social-container">
    
      
    
      
    
      
    
      
    
      
    
  </div>
</div>

        <div class="post-detail">
          <article class="post">
            <h2 class="post-title">
              对预训练模型进行微调
            </h2>
            <div class="post-info">
              <span>
                2023-03-01
              </span>
              <span>
                7 min read
              </span>
              
                <a href="https://voluntexi.github.io/Non44iZPB/" class="post-tag">
                  # NLP
                </a>
              
            </div>
            
              <img class="post-feature-image" src="https://img1.baidu.com/it/u=1732097677,3720483819&amp;fm=253&amp;fmt=auto&amp;app=138&amp;f=PNG?w=500&amp;h=208" alt="">
            
            <div class="post-content-wrapper">
              <div class="post-content" v-pre>
                <p>​	近年来随着自然语言处理技术的不断发展，预训练模型已经成为了近年来最热门的研究方向之一。预训练模型有更好的性能表现。然而，对于刚接触的人来说，阵对预训练模型的训练可能会显得复杂和难以理解。</p>
<!-- more -->
<p>​	因此，本文将以对<strong>BART微调用于文本摘要任务</strong>为例子提供如何对预训练模型+微调的教程，通过本文所提供的示例，相信能够更好地理解和应用预训练模型，从而在自己的研究和项目中取得更好的成果。</p>
<h2 id="什么是预训练微调">什么是预训练+微调</h2>
<p>​	预训练+微调是一种常用的NLP训练的范式。预训练+微调的过程通常可以分为两个阶段：</p>
<p>​	首先，在一个大规模的数据集上训练一个通用的预训练模型，例如BERT、GPT-2等。这个预训练模型的目标是学习一些通用的语言模型或者视觉特征，以便能够应用到各种具体任务中。</p>
<p>​	其次，针对具体的任务，在<strong>相对较小的数据集</strong>上对预训练模型进行微调。可以将预训练模型Fine-tune到文本分类、命名实体识别等子任务中。微调的过程通常是在原来预训练模型的基础上，针对具体任务的特定数据集进行少量的训练，以调整模型的参数，使得模型在具体任务上能够取得更好的表现。</p>
<p>​	叫我们自己实现一个预训练模型是不现实的（没钱、没显卡），但是不用担心，Google、Facebook等大厂会帮我们训练好大模型，我们所需要做的，只需要调用他们在自己的数据集微调就好了。</p>
<h2 id="训练环境搭建">训练环境搭建</h2>
<p>首先我们需要准备好微调环境，你可以在自己的电脑上安装所需要的库，如：Pytorch、Transformers、NLTK等等所必须要库。但是，我觉得可以，但是没有必要。因为随着互联网的发展，我们完全可以使用<strong>云平台</strong>来来协助我们训练模型，接下来的教程，将会依靠百度的云平台<a href="https://aistudio.baidu.com/aistudio/index">飞桨AI Studio - 人工智能学习实训社区</a>来完成</p>
<h3 id="云平台的优点">云平台的优点</h3>
<p>在自己的电脑上运行模型往往会安装大量的库，但是这些库的安装并不像</p>
<pre><code>pip install xxx
</code></pre>
<p>这样一行命令就能够完成安装，往往需要踩过各种坑才能安装成功</p>
<p>而如果使用云平台的话，事情就好办多了，<strong>自带各种深度学习框架，无需下载</strong></p>
<h3 id="如何使用">如何使用</h3>
<p>1.进入<strong>飞桨AI Studio</strong>，注册并登录</p>
<figure data-type="image" tabindex="1"><img src="https://voluntexi.github.io//post-images/1677586801050.png" alt="" loading="lazy"></figure>
<p>2.右上角进入个人中心，在项目里新建一个项目，全部默认设置，最后设置项目名称、标签、描述，有需要可以添加自己的数据集</p>
<figure data-type="image" tabindex="2"><img src="https://voluntexi.github.io//post-images/1677586809322.png" alt="" loading="lazy"></figure>
<p>3.创建好后，点击启动环境，注意基础版没有GPU，所以我们需要选择有GPU的环境</p>
<figure data-type="image" tabindex="3"><img src="https://voluntexi.github.io//post-images/1677586817396.png" alt="" loading="lazy"></figure>
<p>等待环境部署...</p>
<p>4.完善环境的搭建</p>
<p>在文本摘要任务中我们还需要&quot;rouge_score&quot;库来进行ROUGE的计算，wandb库来进行微调的可视化，只需要点击“+”号，进入终端用pip命令安装即可。</p>
<pre><code>pip install rouge_score
pip install wandb
</code></pre>
<p>如此，我们的实验环境便搭建好了</p>
<h2 id="模型的获取">模型的获取</h2>
<p>回到<strong>飞桨AI Studio</strong>，进入模型库，内置了包含许多任务的模型</p>
<figure data-type="image" tabindex="4"><img src="https://voluntexi.github.io//post-images/1677586824511.png" alt="" loading="lazy"></figure>
<p>文本摘要是生成式模型，我们点击“自然语言处理-&gt;文本生成”查看是否有BART模型</p>
<figure data-type="image" tabindex="5"><img src="https://voluntexi.github.io//post-images/1677586830778.png" alt="" loading="lazy"></figure>
<p>没有？其实我们在<a href="https://paddlenlp.readthedocs.io/zh/latest/model_zoo/transformers/BART/contents.html">BART模型汇总 — PaddleNLP 文档</a>，中可以看见，其实PaddleNlp已经内置了BART模型，不需要额外的进行下载。</p>
<p>我们只需要在训练的时候这么写就能导入 <strong>BART</strong> 了</p>
<pre><code class="language-python">from paddlenlp.transformers import BartForConditionalGeneration, BartTokenizer
model = BartForConditionalGeneration.from_pretrained('bart-base')
tokenizer = BartTokenizer.from_pretrained('bart-base')
</code></pre>
<hr>
<p>其实相较于Huggingface，Paddle包含的模型简直少得可怜，但是PaddleNLP并不能直接使用Huggingface里的模型，那怎么办呢？其实我们可以通过转换，将Huggingface里的模型转为PaddleNLP所支持的就可以了</p>
<h2 id="数据集的获取">数据集的获取</h2>
<p>PaddleNLP内置了许多模型，当然也内置了许多的数据库，可以快速的使用API进行调用，具体的数据集可以在此查看：</p>
<p><a href="https://paddlenlp.readthedocs.io/zh/latest/data_prepare/dataset_list.html">PaddleNLP Datasets API — PaddleNLP 文档</a></p>
<p>我们在文本生成一类中发现了我们需要的<strong>摘要数据集</strong></p>
<figure data-type="image" tabindex="6"><img src="https://voluntexi.github.io//post-images/1677586845466.png" alt="" loading="lazy"></figure>
<p>如此，便导入了本次训练的数据集LCSTS</p>
<pre><code class="language-python">from paddlenlp.datasets import load_dataset
train, val = load_dataset('lcsts_new', splits=[&quot;train&quot;, &quot;dev&quot;])
</code></pre>
<p>当然，如果有自己的数据集可以这样导入</p>
<pre><code class="language-python">from paddlenlp.datasets import load_dataset
train_ds, dev_ds = load_dataset(&quot;squad&quot;, data_files=(&quot;my_train_file.json&quot;, &quot;my_dev_file.json&quot;))
test_ds = load_dataset(&quot;squad&quot;, data_files=&quot;my_test_file.json&quot;)
</code></pre>
<h2 id="微调">微调</h2>
<p>以上，我们完成了环境的搭建、模型和数据集的导入，接下来就正式进入微调环节</p>
<p>本次微调我们需要三个文件：</p>
<blockquote>
<p>train.py:训练接口文件,对参数的设置</p>
<p>train_utils.py:模型的加载及其训练具体代码</p>
<p>csl_title_public.py:数据集的加载与处理</p>
</blockquote>
<p>代码在此：<a href="https://github.com/voluntexi/Pre-trainedForSummarization">voluntexi/Pre-trainedForSummarization (github.com)</a></p>
<h3 id="运行">运行</h3>
<p>打开train.py，config中的参数</p>
<pre><code class="language-python">config = {
        'seed': 2022,
        'pretrained_model': 'bart-base', 
        'dataset_name': 'lcsts_new',  
        'train_batch_size': 18,
        'eval_batch_size': 16,
        'save_steps':3000,
        'logging_steps':200,
        'max_source_length': 512,
        'max_target_length': 128,
        'min_target_length': 0,
        'max_steps': -1,
        'warmup_proportion': 0.1,
        'warmup_steps': 0,
        'num_train_epochs': 30,
        'learning_rate': 1e-5,
        'adam_epsilon': 1e-6,
        'weight_decay': 0.01,
        'use_amp': False,
        'scale_loss': 2**15,
        'output_dir': './output',
        'device': 'GPU',
        'ignore_pad_token_for_loss': True
    }
</code></pre>
<p>执行代码</p>
<pre><code>python train.py
</code></pre>
<p>就可以开始训练了，wandb能够将训练过程可视化</p>
<figure data-type="image" tabindex="7"><img src="https://voluntexi.github.io//post-images/1677586864411.png" alt="" loading="lazy"></figure>
<p>可以看到训练完成后的效果并不好，主要有以下几个原因：</p>
<ul>
<li>训练Epoch太少，例子中我只设置的1代的Epoch，当然效果不是很好，在正式的任务中建议还是加大Epoch的数值</li>
<li>本文采用的BART-BASE是在英文上进行预训练的，用于中文肯定效果不好，在正式的任务中对于中文数据集还是需要在Huggingface中寻找使用中文训练的模型</li>
</ul>
<h2 id="总结">总结</h2>
<p>以上就是对预训练模型进行微调的形式，使用到的工具有：百度云平台、PaddlePaddle、PaddleNLP、wandb、NLTK等工具/库</p>
<p>掌握后，后续就可以在此基础上加上其他模型,以更好的提升模型的性能，如BERT+LSTM+ATTEN等模型的拼接训练。</p>

              </div>
              <div class="toc-container">
                <ul class="markdownIt-TOC">
<li>
<ul>
<li><a href="#%E4%BB%80%E4%B9%88%E6%98%AF%E9%A2%84%E8%AE%AD%E7%BB%83%E5%BE%AE%E8%B0%83">什么是预训练+微调</a></li>
<li><a href="#%E8%AE%AD%E7%BB%83%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA">训练环境搭建</a>
<ul>
<li><a href="#%E4%BA%91%E5%B9%B3%E5%8F%B0%E7%9A%84%E4%BC%98%E7%82%B9">云平台的优点</a></li>
<li><a href="#%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8">如何使用</a></li>
</ul>
</li>
<li><a href="#%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%8E%B7%E5%8F%96">模型的获取</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E8%8E%B7%E5%8F%96">数据集的获取</a></li>
<li><a href="#%E5%BE%AE%E8%B0%83">微调</a>
<ul>
<li><a href="#%E8%BF%90%E8%A1%8C">运行</a></li>
</ul>
</li>
<li><a href="#%E6%80%BB%E7%BB%93">总结</a></li>
</ul>
</li>
</ul>

              </div>
            </div>
          </article>
        </div>

        
          <div class="next-post">
            <div class="next">下一篇</div>
            <a href="https://voluntexi.github.io/moca/">
              <h3 class="post-title">
                MoCa
              </h3>
            </a>
          </div>
        

        
          
            <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>

<div id="gitalk-container"></div>

<script>

  var gitalk = new Gitalk({
    clientID: '5f187cc029307ed395eb',
    clientSecret: 'e99c3ac1d57961f0f19a3cd58bc611932d26cd1b',
    repo: 'voluntexi.github.io',
    owner: 'voluntexi',
    admin: ['voluntexi'],
    id: (location.pathname).substring(0, 49),      // Ensure uniqueness and length less than 50
    distractionFreeMode: false  // Facebook-like distraction free mode
  })

  gitalk.render('gitalk-container')

</script>

          

          
        

        <div class="site-footer">
  属于 <a href="https://github.com/voluntexi" target="_blank">@威伦特</a><script async defer src="https://analytics.umami.is/script.js" data-website-id="95248820-3fb8-420e-8f5b-87e136cbc08d"></script>
  <a class="rss" href="https://voluntexi.github.io//atom.xml" target="_blank">
    <i class="ri-rss-line"></i> RSS
  </a>
</div>

      </div>
    </div>

    <script>
      hljs.initHighlightingOnLoad()

      let mainNavLinks = document.querySelectorAll(".markdownIt-TOC a");

      // This should probably be throttled.
      // Especially because it triggers during smooth scrolling.
      // https://lodash.com/docs/4.17.10#throttle
      // You could do like...
      // window.addEventListener("scroll", () => {
      //    _.throttle(doThatStuff, 100);
      // });
      // Only not doing it here to keep this Pen dependency-free.

      window.addEventListener("scroll", event => {
        let fromTop = window.scrollY;

        mainNavLinks.forEach((link, index) => {
          let section = document.getElementById(decodeURI(link.hash).substring(1));
          let nextSection = null
          if (mainNavLinks[index + 1]) {
            nextSection = document.getElementById(decodeURI(mainNavLinks[index + 1].hash).substring(1));
          }
          if (section.offsetTop <= fromTop) {
            if (nextSection) {
              if (nextSection.offsetTop > fromTop) {
                link.classList.add("current");
              } else {
                link.classList.remove("current");    
              }
            } else {
              link.classList.add("current");
            }
          } else {
            link.classList.remove("current");
          }
        });
      });

    </script>
  </body>
</html>
