<html>
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Longformer | 威伦特</title>
<link rel="shortcut icon" href="https://voluntexi.github.io//favicon.ico?v=1769005591865">
<link href="https://cdn.jsdelivr.net/npm/remixicon@2.3.0/fonts/remixicon.css" rel="stylesheet">
<link rel="stylesheet" href="https://voluntexi.github.io//styles/main.css">
<link rel="alternate" type="application/atom+xml" title="Longformer | 威伦特 - Atom Feed" href="https://voluntexi.github.io//atom.xml">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700">



    <meta name="description" content="Longformer是一种用来拓展模型在长序列建模的能力算法，它提出了一种时空复杂度同文本序列长度呈线性关系的Self-Attention，用以保证能够使得模型高效处理长文本。

Tranformer由于采用的是“全连接”型的注意力机制，在..." />
    <meta name="keywords" content="NLP" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
    <script src="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.5.1/build/highlight.min.js"></script>
  </head>
  <body>
    <div class="main">
      <div class="main-content">
        <div class="site-header">
  <a href="https://voluntexi.github.io/">
  <img class="avatar" src="https://voluntexi.github.io//images/avatar.png?v=1769005591865" alt="">
  </a>
  <h1 class="site-title">
    威伦特
  </h1>
  <p class="site-description">
    解码生命
  </p>
  <div class="menu-container">
    
      
        <a href="/" class="menu">
          首页
        </a>
      
    
      
        <a href="/archives" class="menu">
          文章
        </a>
      
    
      
        <a href="/tags" class="menu">
          标签
        </a>
      
    
      
        <a href="/about" class="menu">
          关于
        </a>
      
    
  </div>
  <div class="social-container">
    
      
    
      
    
      
    
      
    
      
    
  </div>
</div>

        <div class="post-detail">
          <article class="post">
            <h2 class="post-title">
              Longformer
            </h2>
            <div class="post-info">
              <span>
                2023-05-18
              </span>
              <span>
                6 min read
              </span>
              
                <a href="https://voluntexi.github.io/Non44iZPB/" class="post-tag">
                  # NLP
                </a>
              
            </div>
            
              <img class="post-feature-image" src="https://pic4.zhimg.com/v2-f903a672ed4c3e0b41ec4ee89a2c8060_1440w.jpg?source=172ae18b" alt="">
            
            <div class="post-content-wrapper">
              <div class="post-content" v-pre>
                <p>Longformer是一种用来拓展模型在长序列建模的能力算法，它提出了一种时空复杂度同文本序列长度呈线性关系的Self-Attention，用以保证能够使得模型高效处理长文本。</p>
<!-- more -->
<p>Tranformer由于采用的是“全连接”型的注意力机制，在处理长文本时有着天然的劣势。因为每一个token都要与其他所有token进行交互。其注意力机制复杂度为 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mi>n</mi><msup><mo>)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">O(n)^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathdefault">n</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span> 。而 Longformer 对Transformer的注意力机制进行了改进，使得时间复杂度降至<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mi>n</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">O(n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathdefault">n</span><span class="mclose">)</span></span></span></span>,极大的节省模型在长文本任务中内存和时间的占用。</p>
<h2 id="概述">概述</h2>
<p>Longformer对注意力机制的改进，具体来说：使每一个token只对固定窗口大小附近的token进行local attention（局部注意力）。并且Longformer针对具体的任务，在原有local attention的基础上增加了一种global attention（全局注意力）。</p>
<p>Longformer在两个字符级语言建模任务上都取得了SOTA的效果。并且作者用Longformer预训练RoBERTa，训练得到的语言模型在多个长文档任务上进行fine-tune后，性能全面超越RoBERTa</p>
<h2 id="longformer">Longformer</h2>
<p>作者共提出了三种新的注意力模式，来降低传统注意力机制的复杂度，分别是<strong>滑窗机制（Sliding window attention）、膨胀滑窗机制（Dilated sliding window）、融合全局信息的滑窗机制（Dilated sliding window）</strong> 。下图展示了传统注意力机制(a)与这三种注意力模式(b,c,d)。<br>
<img src="https://voluntexi.github.io//post-images/1684379605645.png" alt="" loading="lazy"></p>
<h4 id="滑窗机制"><strong>滑窗机制</strong></h4>
<p>滑动窗口的原理，设置一个大小为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span></span></span></span>的窗口，对于每一个token，只对其附近的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span></span></span></span>个token计算attention，复杂度为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mi>n</mi><mo>×</mo><mi>w</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">O(n × w)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathdefault">n</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mclose">)</span></span></span></span> ，其中<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">n</span></span></span></span> 为文本的长度。</p>
<blockquote>
<p>作者认为，根据应用任务的不同，可以对Transformer每一层施以不同的窗口大小，对模型表示能力可能有潜在帮助。</p>
<p>在论文中实验中，作者将窗口<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span></span></span></span> 设置为512，达到了Bert的输入token限制的大小。</p>
</blockquote>
<h4 id="膨胀滑窗机制">膨胀滑窗机制</h4>
<p>对每一个token进行编码时，普通的滑窗机制只能考虑到长度为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span></span></span></span> 的上下文。作者进一步提出空洞滑窗机制，其做法借鉴了空洞卷积的思想,在不增加计算负荷的前提下，拓宽视野范围。在空洞滑窗中，被attented到的两个相邻token之间会存在大小为 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">d</span></span></span></span> 的间隙，因此当计算到第<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>l</mi></mrow><annotation encoding="application/x-tex">l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span></span></span></span> 个token时（即图c中的第<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>l</mi></mrow><annotation encoding="application/x-tex">l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span></span></span></span> 行），此时视野范围可达到<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>l</mi><mo>×</mo><mi>d</mi><mo>×</mo><mi>w</mi></mrow><annotation encoding="application/x-tex">l×d×w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.77777em;vertical-align:-0.08333em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.77777em;vertical-align:-0.08333em;"></span><span class="mord mathdefault">d</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span></span></span></span>。论文实验表明，由于考虑了更加全面的上下文信息，空洞滑窗机制比普通的滑窗机制表现更佳。</p>
<h4 id="融合全局信息的滑窗机制">融合全局信息的滑窗机制</h4>
<p>从字面上就很好理解，即某些token采用滑窗机制，某些token采用全局注意力，而哪些oken获得全局注意力是根据具体任务来决定的，比如，在分类任务中会在[CLS]初添加一个全局注意力（即第一行和第一列全绿）；而在问答任务上会对问题中的所有token添加全局注意力，如图d所示。<br>
那么这种融合全局信息的滑窗Attention具体如何实现呢，我们先来回顾一下经典的Self-Attention，公式如下</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi><mi>t</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo>)</mo><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo>(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msub><msqrt><mi>d</mi></msqrt><mi>k</mi></msub></mfrac><mo>)</mo></mrow><annotation encoding="application/x-tex">attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt d_k})
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">a</span><span class="mord mathdefault">t</span><span class="mord mathdefault">t</span><span class="mord mathdefault">e</span><span class="mord mathdefault">n</span><span class="mord mathdefault">t</span><span class="mord mathdefault">i</span><span class="mord mathdefault">o</span><span class="mord mathdefault">n</span><span class="mopen">(</span><span class="mord mathdefault">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.498031em;vertical-align:-0.9797em;"></span><span class="mord mathdefault">s</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mord mathdefault">t</span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord mathdefault">x</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5183309999999999em;"><span style="top:-2.17778em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.93222em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathdefault" style="padding-left:0.833em;">d</span></span><span style="top:-2.89222em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,
-10,-9.5,-14c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54c44.2,-33.3,65.8,
-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10s173,378,173,378c0.7,0,
35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429c69,-144,104.5,-217.7,106.5,
-221c5.3,-9.3,12,-14,20,-14H400000v40H845.2724s-225.272,467,-225.272,467
s-235,486,-235,486c-2.7,4.7,-9,7,-19,7c-6,0,-10,-1,-12,-3s-194,-422,-194,-422
s-65,47,-65,47z M834 80H400000v40H845z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.10777999999999999em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3286279999999999em;"><span style="top:-2.54252em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15747999999999998em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">Q</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9797em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span></span></span></span></span></p>
<p>即将原始的输入分别映射到了 $ Q,K,V$ 三个空间后进行attention计算，融合全局信息的滑窗机制涉及到两种Attention，分别将这两种attention映射到了两个独立的空间，即使用 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>Q</mi><mi>s</mi></msub><mo separator="true">,</mo><msub><mi>K</mi><mi>s</mi></msub><mo separator="true">,</mo><msub><mi>V</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">Q_s,K_s,V_s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>  来计算滑窗机制，使用<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>Q</mi><mi>g</mi></msub><mo separator="true">,</mo><msub><mi>K</mi><mi>g</mi></msub><mo separator="true">,</mo><msub><mi>V</mi><mi>g</mi></msub></mrow><annotation encoding="application/x-tex">Q_g,K_g,V_g</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">g</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">g</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">g</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>来计算Global Attention。</p>
<h2 id="实验结果">实验结果</h2>
<p>作者在text8和enwiki8两个字符级任务上对Longformer进行了实验。实验中每一层采用了不同的窗口大小，具体来说：底层使用较小的滑窗，以构建局部信息；越上层滑窗越大，以扩大感受野。训练时，理想状况当时是希望使用GPU所能承受最大的w ww和sequence length，但为了加快训练速度，作者采用的是多阶段训练法：从较短的序列长度和窗口大小开始，后续每个阶段将窗口大小和训练长度增加一倍，并将学习率减半。作者一共训练了5个阶段，第一个阶段sequence length是2048，最后一个阶段是23040</p>
<p>实验结果如下所示，Longformer在这两个数据集上皆达到了SOTA（注：测试指标为BPC，bits-per-character，BPC越小性能越好）</p>
<figure data-type="image" tabindex="1"><img src="https://voluntexi.github.io//post-images/1684379615810.png" alt="" loading="lazy"></figure>
<p>作者通过实验，对滑窗机制的设置进行了进一步的讨论。如下表所示：</p>
<p>表中第一组实验（前三行）讨论的是：如果transformer的不同层采用不同窗口大小，是否可以提高性能？实验结果表明，由底层至高层递增窗口大小，可提升性能；递减则反而性能降低。</p>
<p>第二组实验（后两行）是对膨胀滑窗机制的消融实验，证明了增加间隙后的滑窗机制，性能可以有小幅度提升</p>
<figure data-type="image" tabindex="2"><img src="https://voluntexi.github.io//post-images/1684379620475.png" alt="" loading="lazy"></figure>
<h4 id="longformer用于预训练">Longformer用于预训练</h4>
<p>作者采用Longformer的方法在以下四个文档级语料上进行预训练，从而得到适于文档级NLP任务的语言模型。作者并没有完全从头预训练一个随机初始化的模型，而是以RoBERTa为基础，采用MLM(masked language modeling)的方法继续预训练。预训练时，每一层都采用固定的大小为512的滑动窗口，暂不添加global attention。为支持长文本，论文作者把position embedding扩展到了4096个。</p>
<figure data-type="image" tabindex="3"><img src="https://voluntexi.github.io//post-images/1684379624685.png" alt="" loading="lazy"></figure>
<p>预训练结束后，作者在多个文档级任务上进一步对预训练模型做fine-tuning。fine-tune时会根据任务增加global attention，同时设置两套映射矩阵，一套用于局部自注意力，另一套用于全局注意力。实验结果Longformer全面超越了RoBERTa</p>
<figure data-type="image" tabindex="4"><img src="https://voluntexi.github.io//post-images/1684379628609.png" alt="" loading="lazy"></figure>
<h4 id="消融实验">消融实验</h4>
<figure data-type="image" tabindex="5"><img src="https://voluntexi.github.io//post-images/1684379633865.png" alt="" loading="lazy"></figure>
<h2 id="参考文献">参考文献</h2>
<blockquote>
<p><a href="https://arxiv.org/abs/2004.05150v1"> Longformer: The Long-Document Transformer (arxiv.org)</a></p>
<p><a href="https://blog.csdn.net/xixiaoyaoww/article/details/107398795">Longformer：超越RoBERTa，为长文档而生的预训练模型</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/405317918">一文看懂什么是Longformer</a></p>
</blockquote>

              </div>
              <div class="toc-container">
                <ul class="markdownIt-TOC">
<li>
<ul>
<li><a href="#%E6%A6%82%E8%BF%B0">概述</a></li>
<li><a href="#longformer">Longformer</a><br>
*
<ul>
<li><a href="#%E6%BB%91%E7%AA%97%E6%9C%BA%E5%88%B6"><strong>滑窗机制</strong></a></li>
<li><a href="#%E8%86%A8%E8%83%80%E6%BB%91%E7%AA%97%E6%9C%BA%E5%88%B6">膨胀滑窗机制</a></li>
<li><a href="#%E8%9E%8D%E5%90%88%E5%85%A8%E5%B1%80%E4%BF%A1%E6%81%AF%E7%9A%84%E6%BB%91%E7%AA%97%E6%9C%BA%E5%88%B6">融合全局信息的滑窗机制</a></li>
</ul>
</li>
<li><a href="#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C">实验结果</a><br>
*
<ul>
<li><a href="#longformer%E7%94%A8%E4%BA%8E%E9%A2%84%E8%AE%AD%E7%BB%83">Longformer用于预训练</a></li>
<li><a href="#%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C">消融实验</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE">参考文献</a></li>
</ul>
</li>
</ul>

              </div>
            </div>
          </article>
        </div>

        
          <div class="next-post">
            <div class="next">下一篇</div>
            <a href="https://voluntexi.github.io/SimCSE/">
              <h3 class="post-title">
                SimCSE
              </h3>
            </a>
          </div>
        

        
          
            <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>

<div id="gitalk-container"></div>

<script>

  var gitalk = new Gitalk({
    clientID: '5f187cc029307ed395eb',
    clientSecret: 'e99c3ac1d57961f0f19a3cd58bc611932d26cd1b',
    repo: 'voluntexi.github.io',
    owner: 'voluntexi',
    admin: ['voluntexi'],
    id: (location.pathname).substring(0, 49),      // Ensure uniqueness and length less than 50
    distractionFreeMode: false  // Facebook-like distraction free mode
  })

  gitalk.render('gitalk-container')

</script>

          

          
        

        <div class="site-footer">
  属于 <a href="https://github.com/voluntexi" target="_blank">@威伦特</a><script async defer src="https://analytics.umami.is/script.js" data-website-id="95248820-3fb8-420e-8f5b-87e136cbc08d"></script>
  <a class="rss" href="https://voluntexi.github.io//atom.xml" target="_blank">
    <i class="ri-rss-line"></i> RSS
  </a>
</div>

      </div>
    </div>

    <script>
      hljs.initHighlightingOnLoad()

      let mainNavLinks = document.querySelectorAll(".markdownIt-TOC a");

      // This should probably be throttled.
      // Especially because it triggers during smooth scrolling.
      // https://lodash.com/docs/4.17.10#throttle
      // You could do like...
      // window.addEventListener("scroll", () => {
      //    _.throttle(doThatStuff, 100);
      // });
      // Only not doing it here to keep this Pen dependency-free.

      window.addEventListener("scroll", event => {
        let fromTop = window.scrollY;

        mainNavLinks.forEach((link, index) => {
          let section = document.getElementById(decodeURI(link.hash).substring(1));
          let nextSection = null
          if (mainNavLinks[index + 1]) {
            nextSection = document.getElementById(decodeURI(mainNavLinks[index + 1].hash).substring(1));
          }
          if (section.offsetTop <= fromTop) {
            if (nextSection) {
              if (nextSection.offsetTop > fromTop) {
                link.classList.add("current");
              } else {
                link.classList.remove("current");    
              }
            } else {
              link.classList.add("current");
            }
          } else {
            link.classList.remove("current");
          }
        });
      });

    </script>
  </body>
</html>
